from pyspark.sql import SparkSession
from operator import add
spark = SparkSession.builder.master("local[*]").appName('RDDProgram').getOrCreate()
sc = spark.sparkContext

customers = sc.textFile('file:///C:\\Users\\neroy\\pyspark_restaurant\\Customers.csv')
foods = sc.textFile('file:///C:\\Users\\neroy\\pyspark_restaurant\\Foods.csv')
sales = sc.textFile('file:///C:\\Users\\neroy\\pyspark_restaurant\\Sales.csv')
# week2Sales = sc.textFile('file:///C:\\Users\\neroy\\pyspark_restaurant\\week2Sales.csv')

# customers = sc.textFile(r'C:\Users\aborah\Downloads\pyspark_restaurant\Customers.csv')
# foods = sc.textFile(r'C:\Users\aborah\Downloads\pyspark_restaurant\Foods.csv')
# sales = sc.textFile(r'C:\Users\aborah\Downloads\pyspark_restaurant\Sales.csv')
def chunks(xs):
	return (xs[i] for i in range(1, len(xs)))
def chunks_split(xs):
	return (xs[i].split(',') for i in range(len(xs)))
print()
c_data = customers.collect()
c_chunks = list(chunks(c_data))
#print(c_chunks[:10])
print()
ci_chunks = list(chunks_split(c_chunks))
print(ci_chunks[:10])
c_rdd = sc.parallelize(ci_chunks)
print()
s_data = sales.collect()
s_chunks = list(chunks(s_data))
#print(s_chunks[:10])
print()
si_chunks = list(chunks_split(s_chunks))
print(si_chunks[:10])
s_rdd = sc.parallelize(si_chunks)
print()
f_data = foods.collect()
f_chunks = list(chunks(f_data))
#print(f_chunks[:10])
print()
fi_chunks = list(chunks_split(f_chunks))
print(fi_chunks[:10])
f_rdd = sc.parallelize(fi_chunks)
print()
print(c_rdd.collect()[:20])
print()
print(s_rdd.collect()[:20])
print()
print(f_rdd.collect()[:20])
print()
print(c_rdd.join(f_rdd).collect())